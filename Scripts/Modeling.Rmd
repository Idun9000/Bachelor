---
title: "Model"
author: "Tilde Sloth"
date: "2023-11-07"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
# Loading packages
```{r}
pacman::p_load(tidyverse, brms, rsample, yardstick)
```

# Loading data
```{r}
Centrality_DF <- read_csv("../Data/Cleaned/CENTRALITY_DF.csv")
```

# Changing age and education level to ordered factors
```{r}
#' Save age and caregiver_education as ordered factor
education_levels<- c("Some Secondary", "Secondary", "Some College", "College", "Some Graduate", "Graduate")
Centrality_DF <- Centrality_DF %>% 
    mutate(age = factor(age,
                        ordered = TRUE, 
                        levels = sort(unique(age))),
           caregiver_education = factor(caregiver_education,
                                        ordered = TRUE,
                                        levels = education_levels))

# Checking ordered factors
#str(Centrality_DF$age)
#str(Centrality_DF$caregiver_education)

```


# Removing t1
```{r}
Centrality_DF_t2 <- Centrality_DF %>% 
    filter(Timepoint == 2)
```

# Figuring out that there is still duplicates
```{r}
# Remove duplicates based on all columns
Centrality_DF_t2 <- Centrality_DF_t2 %>% distinct()
```


# Setting the formula (fixed and random effects)
```{r}
formula <- brms::bf(Knows_Word ~ 1 + Semantic_centrality + Phonological_centrality + Word_length_centrality + (1|Child_ID))

formula2 <-  brms::bf(Knows_Word ~ 1 + Semantic_centrality + Phonological_centrality + Word_length_centrality + age + (1|Child_ID))

#Conceptually, using trials(1) together with family = binomial is the same as using family = bernoulli. It is more straightforward to use bernoulli since it models binary outcomes with a single trial per observation. Family = binomial models outcomes with 1 or more trials per observation
```

# Setting priors
```{r}
#get_prior(formula, data = Centrality_DF_test)

model_priors <- c(
    prior(normal(0, 3), class = b),
    prior(normal(0, 3), class = Intercept),
    prior(normal(0,2), class = sd)
)

```

## Coefficient prior
The outcome (y) is binary (0 | 1)
The predictors (x) are bounded continuous variables (between 0 and 1)

The coefficient β1 represents the change in the log odds (of the outcome) for a one-unit change in the predictor x. In logistic regression, we look at the outcome in log odds, because the relationship between the predictor and the log odds of the outcome is linear

The odds of an event are calculated as the probability of the event occurring divided by the probability of the event not occurring. If the probability of success is 0.75, the odds are $$\frac{0.75}{0.25} = 3$$

We can convert the log odds change to an odds ratio (which is easier to interpret), by exponentiating the coefficient: Odds Ratio= $$e^{B_1}$$


**The logic**
--> The increase in the predictor variables can max be 1
--> With β1 = 2, increasing x from 0 to 1 multiplies the odds of the outcome by $$Odds ratio = e^2 = 7.389$$
--> With β1 = 3, increasing x from 0 to 1 multiplies the odds of the outcome by $$Odds ratio = e^3 = 20.08$$

--> If the original probability of success at x=0 is 5 %, the odds are $$\frac{0.05}{0.95} = 0.0526$$. Multiplying the odds by 20.09 = new odds of approximately 1.056
--> To find the new probability corresponding to these odds we isolate p: $$P_{new} = \frac{new odds}{1+newodds}$$

# Looking at different probabilties for odds ratio
```{r}
# Define a function to calculate new probability from original probability and odds ratio
calculate_new_probability <- function(original_probability, odds_ratio) {
  original_odds <- original_probability / (1 - original_probability)
  new_odds <- original_odds * odds_ratio
  new_probability <- new_odds / (1 + new_odds)
  return(new_probability)
}

# Set the odds ratio for beta_1 of 3
odds_ratio_for_beta_3 <- exp(3)

# Create a vector of baseline probabilities
baseline_probabilities <- seq(0.05, 0.9, by = 0.05)

# Initialize an empty vector to store the new probabilities
new_probabilities <- numeric(length(baseline_probabilities))

# Loop over the baseline probabilities to calculate the new probabilities
for (i in seq_along(baseline_probabilities)) {
  new_probabilities[i] <- calculate_new_probability(baseline_probabilities[i], odds_ratio_for_beta_3)
}

# Output the results
results <- data.frame(
  Baseline_Probability = baseline_probabilities,
  New_Probability = new_probabilities
)

print(results)

```


## Intercept prior
We should consider centering the intercept prior around a negative value. Centering around 50 % indicates that the baseline probability (when all predictors are 0) for a child knowing a word is 50 %. This is not likely given that the whole amount of words in the data set are 618 and most children do not know half of the words at timepoint 2

```{r}
Percentage_known_words <- Centrality_DF_t2 %>% 
    group_by(Child_ID) %>% 
    summarise(Percentage_known_words = sum(Knows_Word)/length(unique(item_definition)))

hist(Percentage_known_words$Percentage_known_words)
median(Percentage_known_words$Percentage_known_words)
mean(Percentage_known_words$Percentage_known_words)
```
Hmmm okay doing the stats make me reconsider

---

# Splitting the model in train and test data
```{r}
# Splitting based on child_id
set.seed(123)

# Group by 'child' and then use group_split
grouped_data <- Centrality_DF_t2 %>% group_by(Child_ID)

# Get unique groups (children)
children <- grouped_data %>% group_keys()

# Sample the groups
train_child <- sample(children$Child_ID, size = round(nrow(children) * 0.8))

# Split the data based on sampled groups
train_data <- grouped_data %>% filter(Child_ID %in% train_child)
test_data <- grouped_data %>% filter(!(Child_ID %in% train_child))

# Ungroup and create final datasets
Centrality_t2_training <- train_data %>% ungroup()
Centrality_t2_test <- test_data %>% ungroup()

#save
write.csv(Centrality_t2_training, "../Data/Centrality_t2_training.csv")
write.csv(Centrality_t2_test, "../Data/Centrality_t2_test.csv")
```


# Setting model variables 
```{r}
cores <- parallel::detectCores()
chains <- 2
seed <- 123
```


# Creating model
```{r}
Child_specific_model <- brm(formula,
             data = Centrality_t2_training,
             family = bernoulli,
             backend = "cmdstanr",
             prior = model_priors,
             sample_prior = T, 
             iter = 1000,
             warmup = 500,
             cores = cores, 
             chains = chains,
             seed = seed,
             threads = threading(2),
             stan_model_args = list(stanc_options = list("O1"))
             )

# First try: cores = 4, chains = 2, iter = 1000, warmup = 500, prior(normal(0,10)) (6.66 hours)
# Second try: adding threads for more efficient computation + changing from binomial to bernoulli + narrowing priors (normal(0,5)) (21 minutes)
# Third try: narrowing priors again after learning more about coefficients in logistic regression (0,3) & (0,3) and (0,2) (18 min)
# Fourth try: setting cores with parallel:detectCores() (still 18 min but i guess it makes sense. 8 cores will not help when there are only 2 chains)
# Fifth try: now running it on the training data (15 min)
```

# Second model including age
```{r}
Child_specific_model_with_age <- brm(formula2,
             data = Centrality_t2_training,
             family = bernoulli,
             backend = "cmdstanr",
             prior = model_priors,
             sample_prior = T, 
             iter = 1000,
             warmup = 500,
             cores = cores, 
             chains = chains,
             seed = seed,
             threads = threading(2),
             stan_model_args = list(stanc_options = list("O1"))
             )
```


# Here we should have a K-fold or LOO section

# Evaluate classification model
```{r}
# Just for fun checking how well our model does now
# Based on the yardstick package

# use posterior_predict to generate predictions on the test set
posterior_predictive <- posterior_predict(Child_specific_model, newdata = Centrality_t2_test) # matrix where each row corresponds to a prediction for an observation in the test set, and each column corresponds to a draw from the posterior distribution of the predicted probabilities

# for each observation, calculate the mean of all the draws from the posterior predictive distribution for that observation.
Centrality_t2_test$predicted_prob <- apply(posterior_predictive, 2, mean)

# If the predicted probability is greater than 0.5, we predict the class as 1 (knows the word)
Centrality_t2_test$predicted_class <- ifelse(Centrality_t2_test$predicted_prob > 0.5, 1, 0)

# Calculate accuracy - compare predictions with actual outcome
accuracy <- mean(Centrality_t2_test$predicted_class == Centrality_t2_test$Knows_Word)

# Output the performance metrics
print(paste("Accuracy:", accuracy))

```



