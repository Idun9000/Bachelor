---
title: "Model"
author: "Tilde Sloth"
date: "2023-11-07"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
# Loading packages
```{r}
pacman::p_load(tidyverse, brms, rsample, yardstick, rsample, posterior)
```

# Loading data
```{r}
Centrality_DF <- read_csv("../Data/Cleaned/CENTRALITY_DF.csv")

Centrality_DF_t2 <- Centrality_DF %>% 
    filter(Timepoint == 2)

# Remove duplicates based on all columns
Centrality_DF_t2 <- Centrality_DF_t2 %>% distinct()
```

# Setting the formula (fixed and random effects)
```{r}
formula <- brms::bf(Knows_Word ~ 1 + Semantic_centrality + Phonological_centrality + Word_length_centrality + (1|Child_ID))

formula2 <-  brms::bf(Knows_Word ~ 1 + Semantic_centrality + Phonological_centrality + Word_length_centrality + age + (1|Child_ID))

formula3 <- brms::bf(Knows_Word ~ 1 + age + (1|Child_ID))

#Conceptually, using trials(1) together with family = binomial is the same as using family = bernoulli. It is more straightforward to use bernoulli since it models binary outcomes with a single trial per observation. Family = binomial models outcomes with 1 or more trials per observation
```

# Setting priors
```{r}
#get_prior(formula, data = Centrality_DF_test)

model_priors <- c(
    prior(normal(0, 3), class = b),
    prior(normal(0, 3), class = Intercept),
    prior(normal(0,2), class = sd)
)

```

## Coefficient prior
The outcome (y) is binary (0 | 1)
The predictors (x) are bounded continuous variables (between 0 and 1)

The coefficient β1 represents the change in the log odds (of the outcome) for a one-unit change in the predictor x. In logistic regression, we look at the outcome in log odds, because the relationship between the predictor and the log odds of the outcome is linear

The odds of an event are calculated as the probability of the event occurring divided by the probability of the event not occurring. If the probability of success is 0.75, the odds are $$\frac{0.75}{0.25} = 3$$

We can convert the log odds change to an odds ratio (which is easier to interpret), by exponentiating the coefficient: Odds Ratio= $$e^{B_1}$$


**The logic**
--> The increase in the predictor variables can max be 1
--> With β1 = 2, increasing x from 0 to 1 multiplies the odds of the outcome by $$Odds ratio = e^2 = 7.389$$
--> With β1 = 3, increasing x from 0 to 1 multiplies the odds of the outcome by $$Odds ratio = e^3 = 20.08$$

--> If the original probability of success at x=0 is 5 %, the odds are $$\frac{0.05}{0.95} = 0.0526$$. Multiplying the odds by 20.09 = new odds of approximately 1.056
--> To find the new probability corresponding to these odds we isolate p: $$P_{new} = \frac{new odds}{1+newodds}$$

# Looking at different probabilties for odds ratio
```{r}
# Define a function to calculate new probability from original probability and odds ratio
calculate_new_probability <- function(original_probability, odds_ratio) {
  original_odds <- original_probability / (1 - original_probability)
  new_odds <- original_odds * odds_ratio
  new_probability <- new_odds / (1 + new_odds)
  return(new_probability)
}

# Set the odds ratio for beta_1 of 3
odds_ratio_for_beta_3 <- exp(3)

# Create a vector of baseline probabilities
baseline_probabilities <- seq(0.05, 0.9, by = 0.05)

# Initialize an empty vector to store the new probabilities
new_probabilities <- numeric(length(baseline_probabilities))

# Loop over the baseline probabilities to calculate the new probabilities
for (i in seq_along(baseline_probabilities)) {
  new_probabilities[i] <- calculate_new_probability(baseline_probabilities[i], odds_ratio_for_beta_3)
}

# Output the results
results <- data.frame(
  Baseline_Probability = baseline_probabilities,
  New_Probability = new_probabilities
)

print(results)

```


## Intercept prior
We should consider centering the intercept prior around a negative value. Centering around 50 % indicates that the baseline probability (when all predictors are 0) for a child knowing a word is 50 %. This is not likely given that the whole amount of words in the data set are 618 and most children do not know half of the words at timepoint 2

```{r}
Percentage_known_words <- Centrality_DF_t2 %>% 
    group_by(Child_ID) %>% 
    summarise(Percentage_known_words = sum(Knows_Word)/length(unique(item_definition)))

hist(Percentage_known_words$Percentage_known_words)
median(Percentage_known_words$Percentage_known_words)
mean(Percentage_known_words$Percentage_known_words)
```
Hmmm okay doing the stats make me reconsider

---

# Splitting the model in train and test data based on group = Child_ID (no children in the both sets)
```{r}
Centrality_t2_test <- read_csv("../Data/Centrality_t2_test.csv")
Centrality_t2_training <- read_csv("../Data/Centrality_t2_training")

# Adding ordered factors
education_levels<- c("Some Secondary", "Secondary", "Some College", "College", "Some Graduate", "Graduate")
Centrality_t2_test <- Centrality_t2_test %>% 
    mutate(age = factor(age,
                        ordered = TRUE, 
                        levels = sort(unique(age))),
           caregiver_education = factor(caregiver_education,
                                        ordered = TRUE,
                                        levels = education_levels))

Centrality_t2_training <- Centrality_t2_training %>% 
    mutate(age = factor(age,
                        ordered = TRUE, 
                        levels = sort(unique(age))),
           caregiver_education = factor(caregiver_education,
                                        ordered = TRUE,
                                        levels = education_levels))

```


# Setting model variables 
```{r}
cores <- parallel::detectCores()
chains <- 2
seed <- 123
```


# Creating model
```{r}
Child_specific_model <- brm(formula,
             data = Centrality_t2_training,
             family = bernoulli,
             backend = "cmdstanr",
             prior = model_priors,
             sample_prior = T,
             iter = 1000,
             warmup = 500,
             cores = cores,
             chains = chains,
             seed = seed,
             threads = threading(2),
             stan_model_args = list(stanc_options = list("O1")),
             file = "../Models/Child_specific_training1.rds"
             )

# First try: cores = 4, chains = 2, iter = 1000, warmup = 500, prior(normal(0,10)) (6.66 hours)
# Second try: adding threads for more efficient computation + changing from binomial to bernoulli + narrowing priors (normal(0,5)) (21 minutes)
# Third try: narrowing priors again after learning more about coefficients in logistic regression (0,3) & (0,3) and (0,2) (18 min)
# Fourth try: setting cores with parallel:detectCores() (still 18 min but i guess it makes sense. 8 cores will not help when there are only 2 chains)
# Fifth try: now running it on the training data (15 min)

```
# Same model but for k-fold
```{r}
Child_specific_model_k <- brm(formula,
             data = Centrality_DF_t2,
             family = bernoulli,
             backend = "cmdstanr",
             prior = model_priors,
             sample_prior = T,
             iter = 1000,
             warmup = 500,
             cores = cores,
             chains = chains,
             seed = seed,
             threads = threading(2),
             stan_model_args = list(stanc_options = list("O1")),
             file = "../Models/Child_specific_kfold.rds"
             )

```


# Second model including age
```{r}
Child_specific_model_with_age <- brm(formula2,
             data = Centrality_t2_training,
             family = bernoulli,
             backend = "cmdstanr",
             prior = model_priors,
             sample_prior = T,
             iter = 1000,
             warmup = 500,
             cores = cores,
             chains = chains,
             seed = seed,
             threads = threading(2),
             stan_model_args = list(stanc_options = list("O1")),
             file = "../Models/Child_specific_age_training1.rds"
             )

```

# Second model including age but for k-fold
```{r}
Child_specific_model_with_age_k <- brm(formula2,
             data = Centrality_DF_t2,
             family = bernoulli,
             backend = "cmdstanr",
             prior = model_priors,
             sample_prior = T,
             iter = 1000,
             warmup = 500,
             cores = cores,
             chains = chains,
             seed = seed,
             threads = threading(2),
             stan_model_args = list(stanc_options = list("O1")),
             file = "../Models/Child_specific_age_kfold2.rds"
             )

```

# Third model
```{r}
Child_specific_model_only_age <- brm(formula3,
             data = Centrality_t2_training,
             family = bernoulli,
             backend = "cmdstanr",
             prior = model_priors,
             sample_prior = T,
             iter = 1000,
             warmup = 500,
             cores = cores,
             chains = chains,
             seed = seed,
             threads = threading(2),
             stan_model_args = list(stanc_options = list("O1")),
             file = "../Models/Child_specific_only_age1.rds"
             )

```

# Third model k fold
```{r}
Child_specific_model_only_age_k <- brm(formula3,
             data = Centrality_DF_t2,
             family = bernoulli,
             backend = "cmdstanr",
             prior = model_priors,
             sample_prior = T,
             iter = 1000,
             warmup = 500,
             cores = cores,
             chains = chains,
             seed = seed,
             threads = threading(2),
             stan_model_args = list(stanc_options = list("O1")),
             file = "../Models/Child_specific_only_age_kfold.rds"
             )

```



# K-fold: Original child specific model
```{r}
#k_fold_child_specific <- kfold(Child_specific_model_k, K = 5)
#saveRDS(k_fold_child_specific, "../Models/kfold_Child_Specific.rds")

k_fold_child_specific <-  readRDS("../Models/kfold_Child_Specific.rds")

print(k_fold_child_specific)
```

# K-fold: child specific model + age
```{r}
k_fold_child_specific_age <- kfold(Child_specific_model_with_age_k, K = 5)
saveRDS(k_fold_child_specific_age, "../Models/kfold_Child_Specific_with_age1.rds")

#k_fold_child_specific_age <-  readRDS("../Models/kfold_Child_Specific_with_age.rds")
#print(k_fold_child_specific_age)
```
# K-fold: only age
```{r}
#k_fold_child_specific_only_age <- kfold(Child_specific_model_only_age_k, K = 5)
#saveRDS(k_fold_child_specific_only_age, "../Models/kfold_Child_Specific_only_age.rds")

#k_fold_child_specific_only_age <-  readRDS("../Models/kfold_Child_Specific_only_age.rds")
#print(k_fold_child_specific_only_age)
```



# Accuracy: Original child specific model
```{r}
# Generate posterior predictions
posterior_predictions <- brms::posterior_predict(Child_specific_model,
                newdata = Centrality_t2_test,
                re_formula = NULL, allow_new_levels = TRUE,
                sample_new_levels = "uncertainty")
    
# Calculate mean predicted probabilities for each observation
predicted_probs <- apply(posterior_predictions, 2, mean)

# Convert to binary predictions based on a threshold (e.g., 0.5)
predicted_classes <- ifelse(predicted_probs > 0.5, 1, 0)

# Assuming your test data has the actual outcomes in a column named 'Knows_Word'
actual_classes <- Centrality_t2_test$Knows_Word

# Calculate accuracy
accuracy <- mean(predicted_classes == actual_classes)

# Print accuracy
print(paste("Accuracy:", accuracy))
```
Inspiration from: https://www.andrewheiss.com/blog/2021/11/10/ame-bayes-re-guide/#different-kinds-of-average-predictions-with-multilevel-models

# Accuracy: Child specific model + age
```{r}
# Generate posterior predictions
posterior_predictions1 <- brms::posterior_predict(Child_specific_model_with_age,
                newdata = Centrality_t2_test,
                re_formula = NULL, allow_new_levels = TRUE,
                sample_new_levels = "uncertainty")
    
# Calculate mean predicted probabilities for each observation
predicted_probs1 <- apply(posterior_predictions1, 2, mean)

# Convert to binary predictions based on a threshold (e.g., 0.5)
predicted_classes1 <- ifelse(predicted_probs1 > 0.5, 1, 0)

# Assuming your test data has the actual outcomes in a column named 'Knows_Word'
actual_classes1 <- Centrality_t2_test$Knows_Word

# Calculate accuracy
accuracy1 <- mean(predicted_classes1 == actual_classes1)

# Print accuracy
print(paste("Accuracy:", accuracy1))
```


# Accuracy: Child specific only age
```{r}
# Generate posterior predictions
posterior_predictions2 <- brms::posterior_predict(Child_specific_model_only_age,
                newdata = Centrality_t2_test,
                re_formula = NULL, allow_new_levels = TRUE,
                sample_new_levels = "uncertainty")
    
# Calculate mean predicted probabilities for each observation
predicted_probs2 <- apply(posterior_predictions2, 2, mean)

# Convert to binary predictions based on a threshold (e.g., 0.5)
predicted_classes2 <- ifelse(predicted_probs2 > 0.5, 1, 0)

# Assuming your test data has the actual outcomes in a column named 'Knows_Word'
actual_classes2 <- Centrality_t2_test$Knows_Word

# Calculate accuracy
accuracy2 <- mean(predicted_classes2 == actual_classes2)

# Print accuracy
print(paste("Accuracy:", accuracy2))
```

